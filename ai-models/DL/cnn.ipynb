{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOBXdzJNlNkn"
      },
      "source": [
        "# Section 1: Fundamentals in RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC2QWaqZlNkn"
      },
      "source": [
        "## 1.1 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jxtOcietlNkn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lix83hqmlNko"
      },
      "source": [
        "## 1.2 Declare Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "abj8_txllNko"
      },
      "outputs": [],
      "source": [
        "input_size = 5\n",
        "seq_len = 4\n",
        "batch_size = 8\n",
        "hidden_size = 3\n",
        "num_classes = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJe9xbZlNko"
      },
      "source": [
        "## 1.3 Create Random Inputs and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raXJQM1ylNkp",
        "outputId": "59bd7656-d330-4e50-c795-a543b424ddf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([8, 4, 5])\n",
            "Labels: tensor([2, 0, 1, 0, 2, 2, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "# Create random inputs and labels\n",
        "inputs = torch.randn(batch_size, seq_len, input_size, requires_grad=True)\n",
        "random_labels = torch.randint(0, num_classes, (batch_size,))\n",
        "print(\"Input shape:\", inputs.shape)\n",
        "print(\"Labels:\", random_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nrYTTZMlNkp"
      },
      "source": [
        "## 1.4 Initialize Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBm2xslAlNkp",
        "outputId": "09b3ed0c-20b0-495b-a0b2-4de951640696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U shape: torch.Size([5, 3])\n",
            "W shape: torch.Size([3, 3])\n",
            "b shape: torch.Size([3])\n",
            "V shape: torch.Size([3, 3])\n",
            "c shape: torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "# Initialize model parameters with requires_grad=True\n",
        "# U: Input-to-hidden weights [input_size, hidden_size]\n",
        "U = torch.randn(input_size, hidden_size, requires_grad=True)\n",
        "\n",
        "# W: Hidden-to-hidden weights [hidden_size, hidden_size]\n",
        "W = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
        "\n",
        "# b: Hidden bias [hidden_size]\n",
        "b = torch.zeros(hidden_size, requires_grad=True)\n",
        "\n",
        "# V: Output weights [hidden_size, num_classes]\n",
        "V = torch.randn(hidden_size, num_classes, requires_grad=True)\n",
        "\n",
        "# c: Output bias [num_classes]\n",
        "c = torch.zeros(num_classes, requires_grad=True)\n",
        "\n",
        "print(f\"U shape: {U.shape}\")\n",
        "print(f\"W shape: {W.shape}\")\n",
        "print(f\"b shape: {b.shape}\")\n",
        "print(f\"V shape: {V.shape}\")\n",
        "print(f\"c shape: {c.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSMUzostlNkp"
      },
      "source": [
        "## 1.5 Compute Hidden States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLm1lAW4lNkp",
        "outputId": "ea2388a7-3bc8-4b05-b4ba-ae1b1def7138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiddens shape: torch.Size([8, 4, 3])\n",
            "Sample hidden state at last timestep:\n",
            "tensor([-1.0000,  0.8771, -0.9624], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Store hidden states in a list (better for autograd)\n",
        "hidden_states = []\n",
        "\n",
        "# Compute hidden states for each timestep\n",
        "for t in range(seq_len):\n",
        "    if t == 0:\n",
        "        # First timestep: h_0 = tanh(x_0 @ U + b)\n",
        "        h_t = torch.tanh(inputs[:, t, :] @ U + b)\n",
        "    else:\n",
        "        # Subsequent timesteps: h_t = tanh(x_t @ U + h_{t-1} @ W + b)\n",
        "        h_prev = hidden_states[t-1]\n",
        "        h_t = torch.tanh(inputs[:, t, :] @ U + h_prev @ W + b)\n",
        "\n",
        "    hidden_states.append(h_t)\n",
        "\n",
        "# Stack hidden states into a 3D tensor for convenience\n",
        "# This maintains the computation graph\n",
        "hiddens = torch.stack(hidden_states, dim=1)\n",
        "\n",
        "print(\"Hiddens shape:\", hiddens.shape)\n",
        "print(\"Sample hidden state at last timestep:\")\n",
        "print(hiddens[0, -1, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6wszkitlNkp"
      },
      "source": [
        "## 1.6 Compute Logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoOJTCR6lNkp",
        "outputId": "0b49b0ad-893c-4771-80f2-61ffeadd064f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: torch.Size([8, 3])\n",
            "Sample logits:\n",
            "tensor([-2.2371,  1.4742,  1.2297], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Extract the last hidden state\n",
        "last_hidden = hiddens[:, -1, :]  # [batch_size, hidden_size]\n",
        "\n",
        "# Compute logits: logits = h_T @ V + c\n",
        "logits = last_hidden @ V + c  # [batch_size, num_classes]\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Sample logits:\")\n",
        "print(logits[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idJ_k9o8lNkp"
      },
      "source": [
        "## 1.7 Compute Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohqAdb19lNkp",
        "outputId": "2e063d02-d21a-4df8-c959-971f0250056f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Entropy Loss: 1.0832\n"
          ]
        }
      ],
      "source": [
        "# Define cross-entropy loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Compute loss\n",
        "loss = criterion(logits, random_labels)\n",
        "\n",
        "print(f\"Cross-Entropy Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMTdnPg9lNkp"
      },
      "source": [
        "## 1.8 Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjvoDS3ulNkq",
        "outputId": "dd5f5184-aeb0-4a5b-ac30-c791ef0103e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient shapes:\n",
            "U.grad shape: torch.Size([5, 3])\n",
            "W.grad shape: torch.Size([3, 3])\n",
            "b.grad shape: torch.Size([3])\n",
            "V.grad shape: torch.Size([3, 3])\n",
            "c.grad shape: torch.Size([3])\n",
            "\n",
            "Sample gradient values:\n",
            "W.grad sample:\n",
            "tensor([[-0.1413,  0.0902],\n",
            "        [ 0.5686, -0.5391]])\n",
            "\n",
            "b.grad: tensor([ 0.4246, -0.2044, -0.1327])\n",
            "\n",
            "✓ All gradients computed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Perform backpropagation\n",
        "# This computes gradients for all parameters with requires_grad=True\n",
        "loss.backward()\n",
        "\n",
        "# Display computed gradients\n",
        "print(\"Gradient shapes:\")\n",
        "print(f\"U.grad shape: {U.grad.shape}\")\n",
        "print(f\"W.grad shape: {W.grad.shape}\")\n",
        "print(f\"b.grad shape: {b.grad.shape}\")\n",
        "print(f\"V.grad shape: {V.grad.shape}\")\n",
        "print(f\"c.grad shape: {c.grad.shape}\")\n",
        "\n",
        "print(\"\\nSample gradient values:\")\n",
        "print(f\"W.grad sample:\\n{W.grad[:2, :2]}\")\n",
        "print(f\"\\nb.grad: {b.grad}\")\n",
        "\n",
        "# Verify gradients exist\n",
        "assert U.grad is not None, \"U gradient not computed!\"\n",
        "assert W.grad is not None, \"W gradient not computed!\"\n",
        "assert b.grad is not None, \"b gradient not computed!\"\n",
        "assert V.grad is not None, \"V gradient not computed!\"\n",
        "assert c.grad is not None, \"c gradient not computed!\"\n",
        "print(\"\\n✓ All gradients computed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1dLszxlNkq"
      },
      "source": [
        "## 1.9 Manual SGD Update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx3GyEn3lNkq",
        "outputId": "663ea214-14d8-49cf-d650-2decc205989e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters updated successfully!\n",
            "\n",
            "Parameter changes (L2 norm of difference):\n",
            "U change norm: 0.168781\n",
            "W change norm: 0.103965\n",
            "b change norm: 0.048954\n",
            "V change norm: 0.027173\n",
            "c change norm: 0.012614\n",
            "\n",
            "Example: First element of W before and after:\n",
            "Before: 0.238311\n",
            "After:  0.252442\n",
            "Change: 0.014131\n"
          ]
        }
      ],
      "source": [
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Store old values for comparison\n",
        "U_old = U.data.clone()\n",
        "W_old = W.data.clone()\n",
        "b_old = b.data.clone()\n",
        "V_old = V.data.clone()\n",
        "c_old = c.data.clone()\n",
        "\n",
        "# Manual SGD update: θ_new = θ_old - η * ∇θ\n",
        "# Use torch.no_grad() to prevent tracking these operations\n",
        "with torch.no_grad():\n",
        "    U -= learning_rate * U.grad\n",
        "    W -= learning_rate * W.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    V -= learning_rate * V.grad\n",
        "    c -= learning_rate * c.grad\n",
        "\n",
        "print(\"Parameters updated successfully!\")\n",
        "print(\"\\nParameter changes (L2 norm of difference):\")\n",
        "print(f\"U change norm: {torch.norm(U.data - U_old).item():.6f}\")\n",
        "print(f\"W change norm: {torch.norm(W.data - W_old).item():.6f}\")\n",
        "print(f\"b change norm: {torch.norm(b.data - b_old).item():.6f}\")\n",
        "print(f\"V change norm: {torch.norm(V.data - V_old).item():.6f}\")\n",
        "print(f\"c change norm: {torch.norm(c.data - c_old).item():.6f}\")\n",
        "\n",
        "# Show that parameters actually changed\n",
        "print(\"\\nExample: First element of W before and after:\")\n",
        "print(f\"Before: {W_old[0, 0].item():.6f}\")\n",
        "print(f\"After:  {W.data[0, 0].item():.6f}\")\n",
        "print(f\"Change: {(W.data[0, 0] - W_old[0, 0]).item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-VVOQb8lNkq"
      },
      "source": [
        "# Section 2: Deep Learning for Sequential Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEs2Fu-6lNkq"
      },
      "source": [
        "## 2.1 Import Libraries and Set Random Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4PVXgijlNkq",
        "outputId": "728a0200-8361-46af-e286-a9b24e4c0c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def seed_all(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_all(seed=1234)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jjQWDYzlNkq"
      },
      "source": [
        "## 2.2 Download and Preprocess the Data\n",
        "### DataManager Class Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6MMKU7m0lNkq"
      },
      "outputs": [],
      "source": [
        "class DataManager:\n",
        "    \"\"\"\n",
        "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
        "\n",
        "    Attributes:\n",
        "        verbose (bool): Controls verbosity for printing information during data processing.\n",
        "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
        "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
        "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
        "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
        "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
        "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
        "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True, random_state=6789):\n",
        "        self.verbose = verbose\n",
        "        self.max_sentence_len = 0\n",
        "        self.str_questions = list()\n",
        "        self.str_labels = list()\n",
        "        self.numeral_labels = list()\n",
        "        self.maxlen = None\n",
        "        self.numeral_data = list()\n",
        "        self.random_state = random_state\n",
        "        self.random = np.random.RandomState(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
        "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
        "        if verbose:\n",
        "            print(\"Downloaded successfully {}\".format(file_name))\n",
        "\n",
        "    def read_data(self, dir_name, file_names):\n",
        "        self.str_questions = list()\n",
        "        self.str_labels = list()\n",
        "        for file_name in file_names:\n",
        "            file_path= os.path.join(dir_name, file_name)\n",
        "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "                for row in f:\n",
        "                    row_str = row.split(\":\")\n",
        "                    label, question = row_str[0], row_str[1]\n",
        "                    question = question.lower()\n",
        "                    self.str_labels.append(label)\n",
        "                    self.str_questions.append(question[0:-1])\n",
        "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
        "                        self.max_sentence_len = len(self.str_questions[-1])\n",
        "\n",
        "        # turns labels into numbers\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        le.fit(self.str_labels)\n",
        "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
        "        self.str_classes = le.classes_\n",
        "        self.num_classes = len(self.str_classes)\n",
        "        if self.verbose:\n",
        "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
        "            print(self.str_questions[0:5])\n",
        "            print(self.str_labels[0:5])\n",
        "\n",
        "    def manipulate_data(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "        token_ids = []\n",
        "        num_seqs = []\n",
        "        for text in self.str_questions:  # iterate over the list of text\n",
        "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
        "          # Convert tokens to IDs\n",
        "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
        "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
        "          seq_tensor = torch.LongTensor(token_ids)\n",
        "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
        "\n",
        "        # Pad the sequences and create a tensor\n",
        "        if num_seqs:\n",
        "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
        "          self.num_sentences, self.maxlen = self.numeral_data.shape\n",
        "\n",
        "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
        "        train_size = int(self.num_sentences*train_ratio) +1\n",
        "        test_size = int(self.num_sentences*test_ratio) +1\n",
        "        valid_size = self.num_sentences - (train_size + test_size)\n",
        "        data_indices = list(range(self.num_sentences))\n",
        "        random.shuffle(data_indices)\n",
        "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
        "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
        "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
        "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
        "        train_set_labels = torch.from_numpy(train_set_labels)\n",
        "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
        "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
        "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
        "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
        "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
        "        test_set_labels = torch.from_numpy(test_set_labels)\n",
        "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
        "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
        "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
        "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
        "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
        "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
        "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
        "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYAt_5fBlNkr"
      },
      "source": [
        "### Load and Process the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-1YuzK3lNkr",
        "outputId": "025795d5-c298-4e4d-c9d9-38765591bfcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\n",
            "Sample questions and corresponding labels... \n",
            "\n",
            "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
            "['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR']\n"
          ]
        }
      ],
      "source": [
        "print('Loading data...')\n",
        "# Try to download from the official URL, but if it doesn't exist, we'll use the local file\n",
        "#try:\n",
        "#    DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
        "#except:\n",
        "#    print(\"Using local file...\")\n",
        "\n",
        "dm = DataManager()\n",
        "# Use the practice file provided in the /final directory\n",
        "dm.read_data(\"/content/\", [\"train_2000.label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "j3y9xLyqlNkr"
      },
      "outputs": [],
      "source": [
        "dm.manipulate_data()\n",
        "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6ioVXTBlNkr",
        "outputId": "72a34d37-c3b9-40c1-f1d1-93ca6046e1ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch shape: torch.Size([64, 36])\n",
            "Label batch shape: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "for x, y in dm.train_loader:\n",
        "    print(\"Input batch shape:\", x.shape)\n",
        "    print(\"Label batch shape:\", y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rXAYTZUlNkr"
      },
      "source": [
        "# Section 3: Using Word2Vec to Transform Texts to Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxfOddhlNkr"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "RMrCWJlElNkr"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NduyUMIQlNkr"
      },
      "source": [
        "## 3.1 Download Word2Vec Model and Implement get_word_vector Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9RL-JwGlNkr",
        "outputId": "9493b30c-8339-4f3e-9b52-fd425c14c978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Word2Vec model... This may take a few minutes.\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Model downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Download the pretrained Word2Vec model (glove-wiki-gigaword-100)\n",
        "print(\"Downloading Word2Vec model... This may take a few minutes.\")\n",
        "word2vec_model = api.load('glove-wiki-gigaword-100')\n",
        "print(\"Model downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irwrALUllNkr",
        "outputId": "34a45467-6ee9-4bd9-c157-234cc448c48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector for 'computer': [-0.16298   0.30141   0.57978   0.066548  0.45835  -0.15329   0.43258\n",
            " -0.89215   0.57747   0.36375 ]...\n",
            "Vector shape: (100,)\n"
          ]
        }
      ],
      "source": [
        "def get_word_vector(word, model):\n",
        "    \"\"\"\n",
        "    Get the word vector for a given word using the pretrained Word2Vec model.\n",
        "    Returns a zero vector if the word is not in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        word (str): The word to get the vector for\n",
        "        model: The pretrained Word2Vec model\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: 100-dimensional word vector\n",
        "    \"\"\"\n",
        "    try:\n",
        "        vector = model[word]  # Get the word vector from the model\n",
        "    except:\n",
        "        vector = np.zeros(100)  # Return zero vector if word not in vocabulary\n",
        "    return vector\n",
        "\n",
        "# Test the function\n",
        "test_word = \"computer\"\n",
        "test_vector = get_word_vector(test_word, word2vec_model)\n",
        "print(f\"Vector for '{test_word}': {test_vector[:10]}...\")  # Print first 10 dimensions\n",
        "print(f\"Vector shape: {test_vector.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPuLrXhlNkr"
      },
      "source": [
        "## 3.2 Implement get_sentence_vector Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDgOP3IilNks",
        "outputId": "da7897ad-ad2d-49d6-a2eb-9fedf9b7d224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: 'What is machine learning?'\n",
            "Sentence vector shape: (100,)\n",
            "First 10 dimensions: [-0.33702249  0.32326     0.4157875  -0.25846751 -0.1422075   0.1532895\n",
            "  0.16256901  0.17388548  0.207424    0.04120501]\n"
          ]
        }
      ],
      "source": [
        "def get_sentence_vector(sentence, important_score=None, model=None):\n",
        "    \"\"\"\n",
        "    Transform a sentence to a 100-dimensional vector using the pretrained Word2Vec model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The sentence to transform\n",
        "        important_score (list): List of importance scores for each word in the sentence\n",
        "        model: The pretrained Word2Vec model\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: 100-dimensional sentence vector\n",
        "    \"\"\"\n",
        "    # Tokenize the sentence (split by spaces and clean)\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # If no importance scores provided, use equal weights (average)\n",
        "    if important_score is None:\n",
        "        important_score = [1.0] * len(words)\n",
        "\n",
        "    # Make sure important_score has the same length as words\n",
        "    if len(important_score) != len(words):\n",
        "        important_score = [1.0] * len(words)\n",
        "\n",
        "    # Apply softmax to get importance weights\n",
        "    important_score = np.array(important_score)\n",
        "    exp_scores = np.exp(important_score - np.max(important_score))  # Subtract max for numerical stability\n",
        "    important_weight = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "    # Get word vectors and compute weighted sum\n",
        "    feature_vector = np.zeros(100)\n",
        "    for i, word in enumerate(words):\n",
        "        word_vec = get_word_vector(word, model)\n",
        "        feature_vector += important_weight[i] * word_vec\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "# Test the function\n",
        "test_sentence = \"What is machine learning?\"\n",
        "test_sent_vector = get_sentence_vector(test_sentence, model=word2vec_model)\n",
        "print(f\"Sentence: '{test_sentence}'\")\n",
        "print(f\"Sentence vector shape: {test_sent_vector.shape}\")\n",
        "print(f\"First 10 dimensions: {test_sent_vector[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97v91JP7lNks"
      },
      "source": [
        "## 3.3 Transform Training Questions to Feature Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IkVMpcOlNks",
        "outputId": "a582186c-16e3-45d9-f086-ab06ddd908ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform training set to feature vectors...\n",
            "X_train shape: (1601, 100)\n",
            "y_train shape: (1601,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Transform training set to feature vectors...\")\n",
        "\n",
        "# Transform training questions to feature vectors\n",
        "X_train = []\n",
        "for question in dm.train_str_questions:\n",
        "    # Create decaying importance scores: 1.0, 0.9, 0.81, 0.729, ...\n",
        "    words = question.split()\n",
        "    decay_rate = 0.9\n",
        "    important_score = [decay_rate ** i for i in range(len(words))]\n",
        "\n",
        "    # Get the sentence vector\n",
        "    sent_vector = get_sentence_vector(question, important_score=important_score, model=word2vec_model)\n",
        "    X_train.append(sent_vector)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = dm.train_numeral_labels\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SRF-ZrklNk1",
        "outputId": "969366f2-2c46-45d1-d53f-bde6c5d50827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform validation set to feature vectors...\n",
            "X_valid shape: (198, 100)\n",
            "y_valid shape: (198,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Transform validation set to feature vectors...\")\n",
        "\n",
        "# Transform validation questions to feature vectors\n",
        "X_valid = []\n",
        "for question in dm.valid_str_questions:\n",
        "    # Create decaying importance scores: 1.0, 0.9, 0.81, 0.729, ...\n",
        "    words = question.split()\n",
        "    decay_rate = 0.9\n",
        "    important_score = [decay_rate ** i for i in range(len(words))]\n",
        "\n",
        "    # Get the sentence vector\n",
        "    sent_vector = get_sentence_vector(question, important_score=important_score, model=word2vec_model)\n",
        "    X_valid.append(sent_vector)\n",
        "\n",
        "X_valid = np.array(X_valid)\n",
        "y_valid = dm.valid_numeral_labels\n",
        "\n",
        "print(f\"X_valid shape: {X_valid.shape}\")\n",
        "print(f\"y_valid shape: {y_valid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLtoqEiolNk1"
      },
      "source": [
        "## 3.4 Scale Features Using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR3tbuPOlNk1",
        "outputId": "1bb1ce50-c378-4cd7-ed3e-559fd00afd94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaling completed!\n",
            "X_train_scaled range: [-1.000, 1.000]\n",
            "X_valid_scaled range: [-1.218, 1.251]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the MinMaxScaler with feature range (-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "# Fit the scaler on training data and transform both training and validation sets\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "print(\"Scaling completed!\")\n",
        "print(f\"X_train_scaled range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n",
        "print(f\"X_valid_scaled range: [{X_valid_scaled.min():.3f}, {X_valid_scaled.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tz1iGQ6lNk1"
      },
      "source": [
        "## 3.5 Train Logistic Regression and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MhuJhh4lNk1",
        "outputId": "5e3f0c6c-f6df-4161-cc4b-f8260fca762c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression model...\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Make predictions on training and validation sets\n",
        "y_train_pred = lr_model.predict(X_train_scaled)\n",
        "y_valid_pred = lr_model.predict(X_valid_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ym4kWWnlNk1",
        "outputId": "501ecb32-f70f-4bcd-c0ac-721101c6001b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Set Performance:\n",
            "Accuracy: 0.9613\n",
            "\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.9040\n",
            "\n",
            "Validation Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ABBR       1.00      0.50      0.67         2\n",
            "        DESC       0.91      0.86      0.89        36\n",
            "        ENTY       0.89      0.87      0.88        54\n",
            "         HUM       0.96      1.00      0.98        52\n",
            "         LOC       0.84      0.90      0.87        29\n",
            "         NUM       0.88      0.88      0.88        25\n",
            "\n",
            "    accuracy                           0.90       198\n",
            "   macro avg       0.91      0.83      0.86       198\n",
            "weighted avg       0.90      0.90      0.90       198\n",
            "\n",
            "\n",
            "Validation Set Confusion Matrix:\n",
            "[[ 1  1  0  0  0  0]\n",
            " [ 0 31  2  1  1  1]\n",
            " [ 0  2 47  1  2  2]\n",
            " [ 0  0  0 52  0  0]\n",
            " [ 0  0  3  0 26  0]\n",
            " [ 0  0  1  0  2 22]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Evaluate on training set\n",
        "train_accuracy = metrics.accuracy_score(y_train, y_train_pred)\n",
        "print(f\"\\nTraining Set Performance:\")\n",
        "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on validation set\n",
        "valid_accuracy = metrics.accuracy_score(y_valid, y_valid_pred)\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "# Display classification report for validation set\n",
        "print(\"\\nValidation Set Classification Report:\")\n",
        "print(metrics.classification_report(y_valid, y_valid_pred, target_names=dm.str_classes))\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"\\nValidation Set Confusion Matrix:\")\n",
        "print(metrics.confusion_matrix(y_valid, y_valid_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMq46h1blNk1"
      },
      "source": [
        "## 3.6 BaseTrainer Class Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcAgsAQmlNk1",
        "outputId": "40e3c7d7-02c4-4583-a731-0e4872fa7032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaseTrainer class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BaseTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
        "        self.model = model\n",
        "        self.criterion = criterion  # the loss function\n",
        "        self.optimizer = optimizer  # the optimizer\n",
        "        self.train_loader = train_loader  # the train loader\n",
        "        self.val_loader = val_loader  # the valid loader\n",
        "\n",
        "    # the function to train the model in many epochs\n",
        "    def fit(self, num_epochs):\n",
        "        self.num_batches = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "            train_loss, train_accuracy = self.train_one_epoch()\n",
        "            val_loss, val_accuracy = self.validate_one_epoch()\n",
        "            print(\n",
        "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
        "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
        "\n",
        "    # train in one epoch, return the train_acc, train_loss\n",
        "    def train_one_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "        train_loss = running_loss / self.num_batches\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    # evaluate on a loader and return the loss and accuracy\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                batch_loss = self.criterion(outputs, labels)\n",
        "                loss += batch_loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        loss = loss / len(loader)\n",
        "        return loss, accuracy\n",
        "\n",
        "    # return the val_acc, val_loss, be called at the end of each epoch\n",
        "    def validate_one_epoch(self):\n",
        "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
        "      return val_loss, val_accuracy\n",
        "\n",
        "print(\"BaseTrainer class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIdLn5EnlNk1"
      },
      "source": [
        "# Section 4: Text CNN for Sequence Modeling and Neural Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5iqvg5JlNk1"
      },
      "source": [
        "## TextCNN Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDfn8vohlNk2",
        "outputId": "7799a2ce-535b-48c3-9684-9c03a29551f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextCNN model defined successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextCNN(torch.nn.Module):\n",
        "    def __init__(self, embed_size=128, state_size=16, data_manager=None):\n",
        "        super().__init__()\n",
        "        self.data_manager = data_manager\n",
        "        self.embed_size = embed_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "        # Declare the necessary layers\n",
        "        self.embed = nn.Embedding(self.data_manager.vocab_size, self.embed_size)\n",
        "\n",
        "        # Three Conv1D layers with different kernel sizes (3, 5, 7)\n",
        "        # Input channels = embed_size, Output channels = state_size\n",
        "        self.conv1d_1 = nn.Conv1d(in_channels=self.embed_size,\n",
        "                                   out_channels=self.state_size,\n",
        "                                   kernel_size=3,\n",
        "                                   padding=1)\n",
        "\n",
        "        self.conv1d_2 = nn.Conv1d(in_channels=self.embed_size,\n",
        "                                   out_channels=self.state_size,\n",
        "                                   kernel_size=5,\n",
        "                                   padding=2)\n",
        "\n",
        "        self.conv1d_3 = nn.Conv1d(in_channels=self.embed_size,\n",
        "                                   out_channels=self.state_size,\n",
        "                                   kernel_size=7,\n",
        "                                   padding=3)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(state_size * 3, self.data_manager.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding layer: [batch_size, seq_len] -> [batch_size, seq_len, embed_size]\n",
        "        e = self.embed(x)\n",
        "\n",
        "        # Permute for Conv1D: [batch_size, seq_len, embed_size] -> [batch_size, embed_size, seq_len]\n",
        "        e = e.permute(0, 2, 1)\n",
        "\n",
        "        # Apply Conv1D with ReLU activation\n",
        "        # Each convolution produces: [batch_size, state_size, seq_len]\n",
        "        h1 = F.relu(self.conv1d_1(e))\n",
        "        h2 = F.relu(self.conv1d_2(e))\n",
        "        h3 = F.relu(self.conv1d_3(e))\n",
        "\n",
        "        # Apply GlobalMaxPool1D over the sequence dimension\n",
        "        # Output: [batch_size, state_size]\n",
        "        h1 = F.max_pool1d(h1, kernel_size=h1.size(2)).squeeze(2)\n",
        "        h2 = F.max_pool1d(h2, kernel_size=h2.size(2)).squeeze(2)\n",
        "        h3 = F.max_pool1d(h3, kernel_size=h3.size(2)).squeeze(2)\n",
        "\n",
        "        # Concatenate along the feature dimension\n",
        "        # Output: [batch_size, state_size * 3]\n",
        "        h = torch.cat([h1, h2, h3], dim=1)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        h = self.fc(h)\n",
        "        return h\n",
        "\n",
        "print(\"TextCNN model defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5NxYWs2lNk2"
      },
      "source": [
        "## 4.1 Train TextCNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu9UWNRglNk2",
        "outputId": "6ad24b52-eb4c-4ccc-f917-2a66cd908454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training TextCNN model on cuda...\n",
            "Model parameters: 3937878\n",
            "Epoch 1/50\n",
            "26/26 - train_loss: 1.5324 - train_accuracy: 41.4116%                 - val_loss: 1.0089 - val_accuracy: 74.2424%\n",
            "Epoch 2/50\n",
            "26/26 - train_loss: 0.6891 - train_accuracy: 84.0725%                 - val_loss: 0.5240 - val_accuracy: 83.3333%\n",
            "Epoch 3/50\n",
            "26/26 - train_loss: 0.3977 - train_accuracy: 92.0050%                 - val_loss: 0.3381 - val_accuracy: 92.4242%\n",
            "Epoch 4/50\n",
            "26/26 - train_loss: 0.2177 - train_accuracy: 95.5653%                 - val_loss: 0.2813 - val_accuracy: 92.4242%\n",
            "Epoch 5/50\n",
            "26/26 - train_loss: 0.1460 - train_accuracy: 97.6889%                 - val_loss: 0.2306 - val_accuracy: 93.4343%\n",
            "Epoch 6/50\n",
            "26/26 - train_loss: 0.1000 - train_accuracy: 98.8132%                 - val_loss: 0.2069 - val_accuracy: 93.4343%\n",
            "Epoch 7/50\n",
            "26/26 - train_loss: 0.0852 - train_accuracy: 99.6877%                 - val_loss: 0.1800 - val_accuracy: 93.4343%\n",
            "Epoch 8/50\n",
            "26/26 - train_loss: 0.0720 - train_accuracy: 98.6883%                 - val_loss: 0.1688 - val_accuracy: 94.9495%\n",
            "Epoch 9/50\n",
            "26/26 - train_loss: 0.0447 - train_accuracy: 99.8751%                 - val_loss: 0.1632 - val_accuracy: 94.9495%\n",
            "Epoch 10/50\n",
            "26/26 - train_loss: 0.0346 - train_accuracy: 99.9375%                 - val_loss: 0.1520 - val_accuracy: 96.4646%\n",
            "Epoch 11/50\n",
            "26/26 - train_loss: 0.0285 - train_accuracy: 99.9375%                 - val_loss: 0.1529 - val_accuracy: 94.9495%\n",
            "Epoch 12/50\n",
            "26/26 - train_loss: 0.0229 - train_accuracy: 100.0000%                 - val_loss: 0.1489 - val_accuracy: 94.9495%\n",
            "Epoch 13/50\n",
            "26/26 - train_loss: 0.0187 - train_accuracy: 100.0000%                 - val_loss: 0.1394 - val_accuracy: 96.4646%\n",
            "Epoch 14/50\n",
            "26/26 - train_loss: 0.0160 - train_accuracy: 100.0000%                 - val_loss: 0.1421 - val_accuracy: 95.4545%\n",
            "Epoch 15/50\n",
            "26/26 - train_loss: 0.0136 - train_accuracy: 100.0000%                 - val_loss: 0.1352 - val_accuracy: 96.4646%\n",
            "Epoch 16/50\n",
            "26/26 - train_loss: 0.0117 - train_accuracy: 100.0000%                 - val_loss: 0.1364 - val_accuracy: 95.4545%\n",
            "Epoch 17/50\n",
            "26/26 - train_loss: 0.0101 - train_accuracy: 100.0000%                 - val_loss: 0.1325 - val_accuracy: 96.4646%\n",
            "Epoch 18/50\n",
            "26/26 - train_loss: 0.0089 - train_accuracy: 100.0000%                 - val_loss: 0.1302 - val_accuracy: 96.4646%\n",
            "Epoch 19/50\n",
            "26/26 - train_loss: 0.0078 - train_accuracy: 100.0000%                 - val_loss: 0.1294 - val_accuracy: 96.4646%\n",
            "Epoch 20/50\n",
            "26/26 - train_loss: 0.0076 - train_accuracy: 100.0000%                 - val_loss: 0.1268 - val_accuracy: 96.4646%\n",
            "Epoch 21/50\n",
            "26/26 - train_loss: 0.0063 - train_accuracy: 100.0000%                 - val_loss: 0.1273 - val_accuracy: 96.4646%\n",
            "Epoch 22/50\n",
            "26/26 - train_loss: 0.0056 - train_accuracy: 100.0000%                 - val_loss: 0.1265 - val_accuracy: 96.4646%\n",
            "Epoch 23/50\n",
            "26/26 - train_loss: 0.0052 - train_accuracy: 100.0000%                 - val_loss: 0.1250 - val_accuracy: 96.4646%\n",
            "Epoch 24/50\n",
            "26/26 - train_loss: 0.0053 - train_accuracy: 100.0000%                 - val_loss: 0.1236 - val_accuracy: 96.4646%\n",
            "Epoch 25/50\n",
            "26/26 - train_loss: 0.0045 - train_accuracy: 100.0000%                 - val_loss: 0.1165 - val_accuracy: 96.4646%\n",
            "Epoch 26/50\n",
            "26/26 - train_loss: 0.0041 - train_accuracy: 100.0000%                 - val_loss: 0.1164 - val_accuracy: 96.4646%\n",
            "Epoch 27/50\n",
            "26/26 - train_loss: 0.0036 - train_accuracy: 100.0000%                 - val_loss: 0.1164 - val_accuracy: 96.4646%\n",
            "Epoch 28/50\n",
            "26/26 - train_loss: 0.0032 - train_accuracy: 100.0000%                 - val_loss: 0.1159 - val_accuracy: 96.4646%\n",
            "Epoch 29/50\n",
            "26/26 - train_loss: 0.0030 - train_accuracy: 100.0000%                 - val_loss: 0.1156 - val_accuracy: 96.4646%\n",
            "Epoch 30/50\n",
            "26/26 - train_loss: 0.0029 - train_accuracy: 100.0000%                 - val_loss: 0.1146 - val_accuracy: 96.4646%\n",
            "Epoch 31/50\n",
            "26/26 - train_loss: 0.0026 - train_accuracy: 100.0000%                 - val_loss: 0.1142 - val_accuracy: 96.4646%\n",
            "Epoch 32/50\n",
            "26/26 - train_loss: 0.0024 - train_accuracy: 100.0000%                 - val_loss: 0.1132 - val_accuracy: 96.4646%\n",
            "Epoch 33/50\n",
            "26/26 - train_loss: 0.0022 - train_accuracy: 100.0000%                 - val_loss: 0.1131 - val_accuracy: 96.4646%\n",
            "Epoch 34/50\n",
            "26/26 - train_loss: 0.0022 - train_accuracy: 100.0000%                 - val_loss: 0.1118 - val_accuracy: 96.4646%\n",
            "Epoch 35/50\n",
            "26/26 - train_loss: 0.0020 - train_accuracy: 100.0000%                 - val_loss: 0.1108 - val_accuracy: 96.4646%\n",
            "Epoch 36/50\n",
            "26/26 - train_loss: 0.0018 - train_accuracy: 100.0000%                 - val_loss: 0.1105 - val_accuracy: 96.4646%\n",
            "Epoch 37/50\n",
            "26/26 - train_loss: 0.0017 - train_accuracy: 100.0000%                 - val_loss: 0.1104 - val_accuracy: 96.4646%\n",
            "Epoch 38/50\n",
            "26/26 - train_loss: 0.0016 - train_accuracy: 100.0000%                 - val_loss: 0.1096 - val_accuracy: 96.4646%\n",
            "Epoch 39/50\n",
            "26/26 - train_loss: 0.0016 - train_accuracy: 100.0000%                 - val_loss: 0.1097 - val_accuracy: 96.4646%\n",
            "Epoch 40/50\n",
            "26/26 - train_loss: 0.0015 - train_accuracy: 100.0000%                 - val_loss: 0.1094 - val_accuracy: 96.4646%\n",
            "Epoch 41/50\n",
            "26/26 - train_loss: 0.0014 - train_accuracy: 100.0000%                 - val_loss: 0.1086 - val_accuracy: 96.4646%\n",
            "Epoch 42/50\n",
            "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.1083 - val_accuracy: 96.4646%\n",
            "Epoch 43/50\n",
            "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.1082 - val_accuracy: 96.4646%\n",
            "Epoch 44/50\n",
            "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.1080 - val_accuracy: 96.4646%\n",
            "Epoch 45/50\n",
            "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.1081 - val_accuracy: 96.4646%\n",
            "Epoch 46/50\n",
            "26/26 - train_loss: 0.0011 - train_accuracy: 100.0000%                 - val_loss: 0.1079 - val_accuracy: 96.4646%\n",
            "Epoch 47/50\n",
            "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.1076 - val_accuracy: 96.4646%\n",
            "Epoch 48/50\n",
            "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.1081 - val_accuracy: 96.4646%\n",
            "Epoch 49/50\n",
            "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.1095 - val_accuracy: 96.4646%\n",
            "Epoch 50/50\n",
            "26/26 - train_loss: 0.0009 - train_accuracy: 100.0000%                 - val_loss: 0.1069 - val_accuracy: 96.4646%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the TextCNN model\n",
        "text_cnn = TextCNN(data_manager=dm).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(text_cnn.parameters(), lr=0.001)\n",
        "\n",
        "# Create trainer and train the model\n",
        "trainer = BaseTrainer(model=text_cnn,\n",
        "                      criterion=criterion,\n",
        "                      optimizer=optimizer,\n",
        "                      train_loader=dm.train_loader,\n",
        "                      val_loader=dm.valid_loader)\n",
        "\n",
        "print(f\"\\nTraining TextCNN model on {device}...\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in text_cnn.parameters())}\")\n",
        "trainer.fit(num_epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grcexwezlNk2"
      },
      "source": [
        "## 4.2 Evaluate TextCNN on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPW--BvXlNk2",
        "outputId": "096a9786-6c50-43e4-91f2-a56b1c54f9bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Set Performance:\n",
            "test_loss: 0.1846 - test_accuracy: 97.0149%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model on the testing set\n",
        "test_loss, test_acc = trainer.evaluate(dm.test_loader)\n",
        "print(f'\\nFinal Test Set Performance:')\n",
        "print(f'test_loss: {test_loss:.4f} - test_accuracy: {test_acc*100:.4f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
